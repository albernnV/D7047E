{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/shakespear_dataset.txt\", \"r\")\n",
    "text = f.read()\n",
    "\n",
    "data_size = len(text)\n",
    "\n",
    "dictionary = []\n",
    "for i in text:\n",
    "    if i not in dictionary:\n",
    "        dictionary.append(i)\n",
    "#print(dictionary)\n",
    "\n",
    "chunk_len = 100\n",
    "print()\n",
    "dataset = []\n",
    "k = 0\n",
    "for char in text:\n",
    "    ind = [dictionary.index(char)]\n",
    "    dataset.append(ind)\n",
    "\n",
    "dataset = torch.tensor(dataset).to(device)\n",
    "# for i in range(math.floor(len(text)/chunk_len)):\n",
    "#     word = []\n",
    "#     for j in range(k,k+chunk_len):\n",
    "#         word.append(text[j])\n",
    "#     k += chunk_len\n",
    "#     dataset.append(word)\n",
    "#dataset = text.split()\n",
    "#print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'i', 'r', 's', 't', ' ', 'C', 'z', 'e', 'n', ':', '\\n', 'B', 'f', 'o', 'w', 'p', 'c', 'd', 'a', 'y', 'u', 'h', ',', 'm', 'k', '.', 'A', 'l', 'S', 'Y', 'v', '?', 'R', 'M', 'W', \"'\", 'L', 'I', 'N', 'g', ';', 'b', '!', 'O', 'j', 'V', '-', 'T', 'H', 'E', 'U', 'D', 'P', 'q', 'x', 'J', 'G', 'K', 'Q', '&', 'Z', 'X', '3']\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [1],\n",
      "        [4],\n",
      "        [1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def charIndex(x):\n",
    "#     i = 0\n",
    "#     for k in dictionary:\n",
    "#         if x == k:\n",
    "#             return i\n",
    "#         else:\n",
    "#             i += 1\n",
    "\n",
    "# def one_hot(character):\n",
    "#     identity = np.identity(len(dictionary))\n",
    "#     i = charIndex(character)\n",
    "#     one_h = identity[i]\n",
    "#     return np.array(one_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# x = []\n",
    "# y = []\n",
    "# for xe in dataset:\n",
    "#     ind = len(xe)\n",
    "#     y_word = xe[1:]\n",
    "    \n",
    "#     x_word = xe[:ind-1]\n",
    "#     x_embedd = []\n",
    "#     y_embedd = []\n",
    "#     for xc, yc in zip(x_word, y_word):\n",
    "#         i = dictionary.index(xc)\n",
    "#         x_embedd.append(i)\n",
    "#         i = dictionary.index(yc)\n",
    "#         y_embedd.append(i)\n",
    "#     x.append(np.array(x_embedd))\n",
    "#     y.append(np.array(y_embedd))\n",
    "# x = np.array(x)\n",
    "# y = np.array(y)\n",
    "# print(\"First word in x:\")\n",
    "# print(x[0])\n",
    "# print(\"First word in y:\")\n",
    "# print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, train_size=0.5, random_state=None, shuffle=False, stratify=None)\n",
    "\n",
    "#x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.4, train_size=0.6, random_state=None, shuffle=False, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape)\n",
    "# print(x_val.shape)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Hyperparameters ###########\n",
    "hidden_size = 200   # size of hidden state\n",
    "seq_len = 100       # length of LSTM sequence\n",
    "num_layers = 3      # num of layers in LSTM layer stack\n",
    "learning_rate = 0.001          # learning rate\n",
    "weight_decay = 1e-5            #weight decay\n",
    "epochs = 200        # max number of epochs\n",
    "drop_prob = 0.5\n",
    "vocab_size = len(dictionary)  #length pf dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, drop_prob):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, input_size)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        #self.drop_out = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        embedds = self.embedding(x)\n",
    "        out, hidden_state = self.lstm(embedds, hidden_state)\n",
    "        #out = self.drop_out(out)\n",
    "        out = self.fc(out)\n",
    "        return out, (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)),\n",
    "                    Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "charRnn = CharRNN(vocab_size, vocab_size, hidden_size, num_layers, drop_prob)\n",
    "charRnn = charRnn.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(charRnn.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 1.79236666675405839631347656\n",
      "-----------------------------------------------------------------\n",
      "ich-bathing up, good but you, ilgeN, out\n",
      "As 'traelhed do alrove should\n",
      "would should show'd mennish! \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.48426003137276561209182748\n",
      "-----------------------------------------------------------------\n",
      "istority;--\n",
      "\n",
      "GONZALO:\n",
      "What you know my wrices-watch.\n",
      "\n",
      "SEBASTIAN:\n",
      "\n",
      "ANTONIO:\n",
      "Who cause yourself.\n",
      "\n",
      "GONZ\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.40807471637243566351852417\n",
      "-----------------------------------------------------------------\n",
      "-withed without vice of upon the foul more;\n",
      "To't I our talk me.\n",
      "\n",
      "ALONSO:\n",
      "\n",
      "SEBASTIAN:\n",
      "Foul things not\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.36846380051722533068847656\n",
      "-----------------------------------------------------------------\n",
      "roaz me kind or commend\n",
      "cut to-morrow, gentling at awake; trall of least, as nothing I\n",
      "make this, Wi\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.34229533918489845402297974\n",
      "-----------------------------------------------------------------\n",
      "um part.\n",
      "\n",
      "GONZALO:\n",
      "a tuth to see, fair you, my lord:\n",
      "It was part better mean o' the strange!\n",
      "\n",
      "ANTONI\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.32215921851345145159454346\n",
      "-----------------------------------------------------------------\n",
      " capanly.\n",
      "\n",
      "SEBASTIAN:\n",
      "Ay, achieve who wishing that, but as you go.\n",
      "\n",
      "COKINAND:\n",
      "Yea, falling depEns; s\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.30637916402304587717819214\n",
      "-----------------------------------------------------------------\n",
      "uld with you; do you a vmiling\n",
      "Thy bodyings day in the citatious bose.\n",
      "\n",
      "SEBASTIAN:\n",
      "Sir, this sad hat\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.29363322733101233848495483\n",
      "-----------------------------------------------------------------\n",
      "e to the needlingly which.\n",
      "\n",
      "ANTONIO:\n",
      "You lie foul conceiving\n",
      "Bine all this's cockllancured, I do;\n",
      "Ho\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.28533680735816122044296265\n",
      "-----------------------------------------------------------------\n",
      "NIO:\n",
      "We prove.\n",
      "\n",
      "ALONSO:\n",
      "Hethich is a lestily; if my rest importune.\n",
      "\n",
      "ANTONIO:\n",
      "Nature? Yet I sir.\n",
      "\n",
      "AL\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.277513050994541427296829224\n",
      "-----------------------------------------------------------------\n",
      "spoiled. ring out of duke appear,\n",
      "Do what thou thill to aceing not but lands, all,\n",
      "Nake of content o\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.268381119645530107469558716\n",
      "-----------------------------------------------------------------\n",
      "LUSTANDA:\n",
      "Alack, and mess, gong, being too enal, it says' office\n",
      "Son;\n",
      "And let them so great all heir\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.257095185393315749436569214\n",
      "-----------------------------------------------------------------\n",
      "n use again, follows.\n",
      "\n",
      "SEBASTIAN:\n",
      "A maid's wench not stower and dings to Boin'd\n",
      "With name ends.\n",
      "\n",
      "PRO\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.258978982864447591105651855\n",
      "-----------------------------------------------------------------\n",
      "t his ifols, how it is the atces?\n",
      "\n",
      "SEBASTIAN:\n",
      "This son Your biddy death in the continue of Milan.\n",
      "\n",
      "G\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.251264240114749605503082275\n",
      "-----------------------------------------------------------------\n",
      "et, now well see the man\n",
      "O' the ill varments of the ass in good surate.\n",
      "\n",
      "SEBASTIAN:\n",
      "Soly, sanctifor.\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.249177518002518674978637695\n",
      "-----------------------------------------------------------------\n",
      "SON:\n",
      "The truth willing and ficknings woldy and eyes,\n",
      "And sleep win the lungs whom I like a very\n",
      "new \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.242281909214694647051620483\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "and my strength is it. Sea, I hope!\n",
      "\n",
      "NIABHIANA:\n",
      "I\n",
      "'ched of all words and in esteem though to find\n",
      "T\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.240842527838915287109756472\n",
      "-----------------------------------------------------------------\n",
      "ey.\n",
      "\n",
      "AcTONIO:\n",
      "When, I warrant you use it?\n",
      "\n",
      "ANTONIO:\n",
      "Why, in my heed with\n",
      "since' she was fornited out\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.237215066056588414772033691\n",
      "-----------------------------------------------------------------\n",
      " miser'd rargers;\n",
      "Am nothing means a thousand, when you do heaven yet and the\n",
      "news in, and or metal.\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.234575144845024844013214111\n",
      "-----------------------------------------------------------------\n",
      "isto's oddle,\n",
      "I press the horrible.\n",
      "\n",
      "SEBASTIAN:\n",
      "Tyacher, good, Urbdel, is the wire of such as you\n",
      "An\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.231922229907763570552444458\n",
      "-----------------------------------------------------------------\n",
      "ith hymeltly morning; more than I did assure yourself;\n",
      "The did she a swift-love whiltings in nage,\n",
      "A\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.228219108224332482345581055\n",
      "-----------------------------------------------------------------\n",
      "s came:\n",
      "What art it?\n",
      "\n",
      "GONZALO:\n",
      "Good, tinher! he fooler, I lock him.\n",
      "\n",
      "ANTONIO:\n",
      "I' thy most prest if m\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.226398595419633745428085327\n",
      "-----------------------------------------------------------------\n",
      "ey\n",
      "false-boding of my laudio, awake; what doth he\n",
      "blush a kind of orian, our words for our favour;\n",
      "F\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.223851452443051703514099126\n",
      "-----------------------------------------------------------------\n",
      " to goes. The goodness wild\n",
      "offinance.\n",
      "\n",
      "ANTONIO:\n",
      "Go, fair such a odly assisting icellon move.\n",
      " SEBAS\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.220591817261937907901763916\n",
      "-----------------------------------------------------------------\n",
      "to me feel, so; for who\n",
      "walk\n",
      "Go prison, speak; for good-each, plughanca!\n",
      "How, having comfort in chri\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.218277155583422864536285414\n",
      "-----------------------------------------------------------------\n",
      ";\n",
      "And lebribetual of his down, that, agree, would his drush,\n",
      "Who should be yurnty ever: nothing bett\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.216654404010471659540176392\n",
      "-----------------------------------------------------------------\n",
      " sounds mowing-coliftent'd, and contepute yourself,\n",
      "And say' this grate, ring, a wildmally fairer.\n",
      "B\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.212679997127943280927276611\n",
      "-----------------------------------------------------------------\n",
      " my dinners,\n",
      "The Heaved slave, sol.\n",
      "When it I seem at Pinament: and the best of bolt\n",
      "And to off begi\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.209671650832408885846710205\n",
      "-----------------------------------------------------------------\n",
      "O:\n",
      "They remember nothing.\n",
      "\n",
      "CEPTISS:\n",
      "Faithfully\n",
      "I'ld kiss'd thee two forces in his sword in Sea.\n",
      "Foot\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.211148139095024387244796753\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Kinds and yet out them and will be winn'd\n",
      "As lawful mustal turn our hearts for joy.\n",
      "\n",
      "SEBASTIAN:\n",
      "Wel\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.206964531356760527659225464\n",
      "-----------------------------------------------------------------\n",
      "o winten, both shall by owe.\n",
      "\n",
      "SEBASTIAN:\n",
      "\n",
      "ANTONIO:\n",
      "I prays not shortly let me could not,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No courthal\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.207164393180605104520797738\n",
      "-----------------------------------------------------------------\n",
      "could not between him.\n",
      "\n",
      "ANTONIO:\n",
      "They o'er as beast, and make him then repairs,\n",
      "of blots and, to a l\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.204061882812047006752014163\n",
      "-----------------------------------------------------------------\n",
      "up I straight\n",
      "To her.\n",
      "\n",
      "ANTONIO:\n",
      "Was center in the name, and piece my father\n",
      "Wonth with blueies make \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.203745496820294367362976074\n",
      "-----------------------------------------------------------------\n",
      "hreal; would I kept them.\n",
      "\n",
      "GONZALO:\n",
      "I could be, to\n",
      "impompty mine\n",
      "Some hour sorrow,--up not, I aloud;\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.199359368211237886961364746\n",
      "-----------------------------------------------------------------\n",
      "ry.\n",
      "\n",
      "ANGONO:\n",
      "How! courace up, no! I same stread, that indeed i' the king,\n",
      "Which they shall or name f\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.2012979145131075889194488533\n",
      "-----------------------------------------------------------------\n",
      " me again,\n",
      "Or never wrong him, and his as it never should\n",
      "Their frottunes: draw about the performOna\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.199367938020951254999160767\n",
      "-----------------------------------------------------------------\n",
      "s, no more son, nor stands mine\n",
      "Action flap. She seen not the heaving warrants! All\n",
      "fresh of sullenc\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.197963233797002877991104126\n",
      "-----------------------------------------------------------------\n",
      "Romeosomether?\n",
      "\n",
      "GONZALO:\n",
      "Nay, I warrant you,--\n",
      "\n",
      "SEBASTIAN:\n",
      "Ked that thou hear; we shall let me, wert\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.196012288457576246492385864\n",
      "-----------------------------------------------------------------\n",
      "rake! so, hold.\n",
      "Awake you well most green!\n",
      "\n",
      "SEBASTIAN:\n",
      "It jeason house well.\n",
      "\n",
      "ANTONIO:\n",
      "But were you \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.194442831170213829169464111\n",
      "-----------------------------------------------------------------\n",
      "heels undurate\n",
      "yet this day widow, and way well.\n",
      "\n",
      "SEBASTIAN:\n",
      "Thus this is hence'ther.\n",
      "Where'e hope o\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.193321831945901467980194092\n",
      "-----------------------------------------------------------------\n",
      "past 'fore yourselves: for; My lord.\n",
      "\n",
      "ANTONIO:\n",
      "And to them did not be something wrung, at jade,\n",
      "I le\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.188399557446104959095001228\n",
      "-----------------------------------------------------------------\n",
      " widow inUsience, sound minister'd, trusts, his dressers,\n",
      "In this unsoil heard me, comfort, my Lord\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.191903286281427670702743539\n",
      "-----------------------------------------------------------------\n",
      "IENIO:\n",
      "Treast; thou, the king's name or two.\n",
      "\n",
      "ANTONIO:\n",
      "Hadst, too noble.\n",
      "\n",
      "GONZALO:\n",
      "No, sir, on Gon-a\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.188387353158602988059997559\n",
      "-----------------------------------------------------------------\n",
      "rried.\n",
      "\n",
      "ANTONIO:\n",
      "Villain: 'tis a selful man on't.\n",
      "\n",
      "GONZALO:\n",
      "\n",
      "ALONSO:\n",
      "Thou art, elry out; we thrive n\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.189193543052391268289184572\n",
      "-----------------------------------------------------------------\n",
      "sleep;\n",
      "Good lord, comes, and afforse me.\n",
      "\n",
      "ANTONIO:\n",
      "Hence! some only done.\n",
      "Pawder! Whe to my boy, my \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.189870597118435924610137947\n",
      "-----------------------------------------------------------------\n",
      "housand loss, rident of gentlewomen;\n",
      "And then and peace awake.\n",
      "\n",
      "GONZALO:\n",
      "Of the king more than themi\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.186514403171671791400909424\n",
      "-----------------------------------------------------------------\n",
      "well my mother hide,\n",
      "Why so too viand'st our dignity: sail in\n",
      "every sension pains age.\n",
      "\n",
      "ANTONIO:\n",
      "And\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.181976152868706419977188118\n",
      "-----------------------------------------------------------------\n",
      "our brave and your nobleness\n",
      "He end they almost noncled to be asleep.\n",
      "\n",
      "GONZALO:\n",
      "It hath now in the r\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.184514733122077818457412728\n",
      "-----------------------------------------------------------------\n",
      "e ward\n",
      "What's open gound or dispite good and dish,\n",
      "Who doth gone on the gods and dry his poat\n",
      "Into t\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.185023704559744545221328735\n",
      "-----------------------------------------------------------------\n",
      "name of silttion overpeaping owe\n",
      "As partrillays.\n",
      "\n",
      "SEBASTIAN:\n",
      "Well caite thee aZout,--\n",
      "\n",
      "SEBASTIAN:\n",
      "Bu\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.180815917354501329051971436\n",
      "-----------------------------------------------------------------\n",
      "ed, a rark.\n",
      "\n",
      "SEBASTIAN:\n",
      "We'll sauce hithy governse, in Making,--\n",
      "\n",
      "GONZALO:\n",
      "No;\n",
      "I am but limital wate\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.182411699666791335554885864\n",
      "-----------------------------------------------------------------\n",
      " attend it.\n",
      "\n",
      "ANTANIO:\n",
      "I fear not so; your best of us, with the\n",
      "celling safely teach'd your rate; may\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.174532017397429482534027127\n",
      "-----------------------------------------------------------------\n",
      "eenke\n",
      "but asleases.--\n",
      "\n",
      "ALONSO:\n",
      "Sir, in Foresker, that had at the world in\n",
      "prevention of mine, you be\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.181338904337488406545639038\n",
      "-----------------------------------------------------------------\n",
      "her to this face.\n",
      "\n",
      "SLIONA:\n",
      "Ere yet in all mercated, one rest was daisty.\n",
      "\n",
      "GONZALO:\n",
      "Do not my consola\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.179336224507186872754669194\n",
      "-----------------------------------------------------------------\n",
      "he least\n",
      "I love not; brave number.\n",
      "\n",
      "ALONSO:\n",
      "My brother, let me spirous office speak.'\n",
      "\n",
      "SEARNIA:\n",
      "Than\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.179171754207617548681259155\n",
      "-----------------------------------------------------------------\n",
      "Vour\n",
      "And thine, his did foul wave or tender.\n",
      "\n",
      "ANTONIO:\n",
      "Take brother from my father.\n",
      "\n",
      "ALONSO:\n",
      "Should \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.178584286212750978126525889\n",
      "-----------------------------------------------------------------\n",
      ",\n",
      "My life consent thou canst not druw; so, follow\n",
      "Then all afiends and garments shall find a most ir\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.176498952280039591602706915\n",
      "-----------------------------------------------------------------\n",
      " neck, the beach mulion of such father.\n",
      "\n",
      "ANTONIO:\n",
      "\n",
      "LUCENTIO:\n",
      "My most obedience say thou art, tenderu\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.172228848780258648522567753\n",
      "-----------------------------------------------------------------\n",
      "ienna, i my marks\n",
      "I might dine sak? I'll not codley him.\n",
      "\n",
      "ANTONIO:\n",
      "Call up a quartorisan, yout-for y\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.174454323590031507832336426\n",
      "-----------------------------------------------------------------\n",
      "tell him; such pen\n",
      "Plain there all.\n",
      "\n",
      "ANTONIO:\n",
      "We perfend not of amendant bidding Ancetion.\n",
      "\n",
      "ANTONIO:\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.175918411612136655401229858\n",
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nly thou vouchest. But if you hear you,\n",
      "Good father, do.\n",
      "\n",
      "GONZALO:\n",
      "To do thee in thy life;--your tho\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.175444851319885930834579468\n",
      "-----------------------------------------------------------------\n",
      "ess nothing\n",
      "Wisely, the opiness of the travellest cowand.\n",
      "Would my follow home, dibble open!\n",
      "\n",
      "CENSS:\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.173027384230922224546432495\n",
      "-----------------------------------------------------------------\n",
      "hee!\n",
      "\n",
      "First Sicina:\n",
      "O dost thou quit my villany? I have reason bat\n",
      "As millsto-mala! dead abides, mad\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.172664545554596571393966675\n",
      "-----------------------------------------------------------------\n",
      "of the faithoon so.\n",
      "Squll, can fine shepher, is men pure; one herthind too much\n",
      "And have forsaken th\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.169048235863834618669128418\n",
      "-----------------------------------------------------------------\n",
      "rm?\n",
      "\n",
      "ANTONIO:\n",
      "Married her drunks; I would have try no means.\n",
      "\n",
      "ALONSO:\n",
      "If you would not know not with\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1706587435829792245529174854\n",
      "-----------------------------------------------------------------\n",
      "ADRIAN:\n",
      "Tell me, gentle, and mave the firrynet\n",
      "Which should be crossing with them.\n",
      "\n",
      "ALONSO:\n",
      "I know n\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.169456295003657698826980596\n",
      "-----------------------------------------------------------------\n",
      "ain; but this begnant space; and\n",
      "You was dumberny light what servant studde at labble.\n",
      "\n",
      "ANTONIO:\n",
      "Pin\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.167998288646788407274246216\n",
      "-----------------------------------------------------------------\n",
      "uspacted, the earls able and\n",
      "perform'd to prison!\n",
      "\n",
      "ANTONIO:\n",
      "His father,\n",
      "All wrong with care of me:--\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.168867101634467939048767093\n",
      "-----------------------------------------------------------------\n",
      "uler commonwealthen, sunder,--\n",
      "\n",
      "GONZALO:\n",
      "I have done.\n",
      "\n",
      "PELRO:\n",
      "God and a brave travocoun and\n",
      "Whits di\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1674022423005752563041687016\n",
      "-----------------------------------------------------------------\n",
      " dinning at,\n",
      "How you unknowly in their softs, conceive\n",
      "To his angry.\n",
      "\n",
      "ANTONIO:\n",
      "Whether you woo'd tha\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.164677354078026641788482666\n",
      "-----------------------------------------------------------------\n",
      "doth our tellegrety be\n",
      "Avillands not; one being swanded with you,\n",
      "Or else of his offer, widows, in m\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.167322275665943966550827026\n",
      "-----------------------------------------------------------------\n",
      "othing I'll; not honour.\n",
      "\n",
      "NORTONIO:\n",
      "'Tis in my farther.\n",
      "\n",
      "GONZALO:\n",
      "\n",
      "ANTOLIO:\n",
      "Go, ha! sure him!\n",
      "\n",
      "SEBAS\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.164747070455983866334915161\n",
      "-----------------------------------------------------------------\n",
      "s.'\n",
      "\n",
      "SABPH:\n",
      "Sir, I all, sir, 'judge off.\n",
      "\n",
      "SEBASTIAN:\n",
      "I long than all eresom'd; he willst thy brave e\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.166026962762198418233108525\n",
      "-----------------------------------------------------------------\n",
      "NTIO:\n",
      "Pardon,\n",
      "Have I deliver when thou mightht, I cannot\n",
      "swear it, man; to play now--\n",
      "\n",
      "ANTONIO:\n",
      "I do\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.164797084802663252654266357\n",
      "-----------------------------------------------------------------\n",
      "e it, as to detirral heart!\n",
      "\n",
      "ANTONIO:\n",
      "Of 'incaps our hadst a torment I'll prate,\n",
      "So or botyatoons, e\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.164904902208592239282608032\n",
      "-----------------------------------------------------------------\n",
      "ou hide me enough,\n",
      "While you make ment with: a fool, are they seek.\n",
      "\n",
      "ANTONIO:\n",
      "Upon my cause, in an o\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.162629244389645588821411133\n",
      "-----------------------------------------------------------------\n",
      "eth coracts\n",
      "Worse upon my master, I do remember\n",
      "Sweet hosten, with and that thou speak'st,\n",
      "Postering\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.163738364816556537419128418\n",
      "-----------------------------------------------------------------\n",
      ", and of thy countryman, you paose on the church,\n",
      "Would fall that idle heaves and.\n",
      "\n",
      "ANTONIO:\n",
      "Tarry n\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.158321215803287871612930298\n",
      "-----------------------------------------------------------------\n",
      " and head himself.\n",
      "\n",
      "SEBASTIAN:\n",
      "Your word, so it is.\n",
      "\n",
      "ANTONIO:\n",
      "Woman 's, in foul truth; a coucidant\n",
      "I\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.160465249932529408599853516\n",
      "-----------------------------------------------------------------\n",
      "at I have weam\n",
      "To thee thereouwe, thou dman.\n",
      "\n",
      "ADRIAN:\n",
      "I see they drown, but watch win thee; inform'd\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.161133969513106402746200562\n",
      "-----------------------------------------------------------------\n",
      "im.\n",
      "\n",
      "ALONSO:\n",
      "The elveting face, for they come our duke,\n",
      "Was sain of main thing.\n",
      "\n",
      "GONZALO:\n",
      "Come on, s\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1593315715342158040081024177\n",
      "-----------------------------------------------------------------\n",
      "he tempest of Whom Whose is.\n",
      "\n",
      "SEBASTIAN:\n",
      "Nay, he's a forter'd dear not.\n",
      "\n",
      "SEBASTIAN:\n",
      "The own god once\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.162046876903244227388381958\n",
      "-----------------------------------------------------------------\n",
      "ot we then pour in sulchsame.\n",
      "\n",
      "ALONSO:\n",
      "Tongue! all one; a woman; or to honour invocile.\n",
      "Right thy go\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.160362849868937624529647827\n",
      "-----------------------------------------------------------------\n",
      "ce and all gings vouch it,\n",
      "Which sweet sir vock with grue-desties should be service,\n",
      "Who thou dost a\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.158454074727508740311813354\n",
      "-----------------------------------------------------------------\n",
      "o side, likewisance,--quist your high so\n",
      "fear, to commonnell figret that he does\n",
      "To make the best of\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.158066079160787334856796265\n",
      "-----------------------------------------------------------------\n",
      " the business,\n",
      "And speak, two in my benocriments! widow!\n",
      "\n",
      "ANTONIO:\n",
      "No, my good lorm, I say:\n",
      "Incline \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1585247313691946514617919927\n",
      "-----------------------------------------------------------------\n",
      " xine we are how'd of mine's.\n",
      "Tend in such names on so follow'd his thought,\n",
      "Voil, sir, more traded \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.158767734363903646919250488\n",
      "-----------------------------------------------------------------\n",
      "you follow, title.\n",
      "\n",
      "ALONSO:\n",
      "Arise ashame on two.\n",
      "\n",
      "SEBASTIAN:\n",
      "Widow, what hast thou now, what show?\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.155714574682544887854766846\n",
      "-----------------------------------------------------------------\n",
      " to be senish'd.\n",
      "So, no.\n",
      "\n",
      "SEBASTIAN:\n",
      "What, is it two Pedasward good and advo?\n",
      "\n",
      "CALIBAN:\n",
      "He swinged i\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.157462657636284878855895996\n",
      "-----------------------------------------------------------------\n",
      "me: Rave hang.\n",
      "\n",
      "SEBASTIAN:\n",
      "I must attend your loss my good and word.\n",
      "\n",
      "ANTONIO:\n",
      "Expound me; when as v\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 1.155758945143277313724136353\n",
      "-----------------------------------------------------------------\n",
      "he marriage stroke thee! You tive to nothing.\n",
      "\n",
      "ORTENSON:\n",
      "Sir, thy wait should be see that bid so hom\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.156054636475046693650054932\n",
      "-----------------------------------------------------------------\n",
      " a thing that\n",
      "Will keep his perverge.\n",
      "\n",
      "ANTONIO:\n",
      "You are, my quarter,--\n",
      "\n",
      "SEBASTIAN:\n",
      "O heavy joy!\n",
      "\n",
      "ANT\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.154281236174544307225418095\n",
      "-----------------------------------------------------------------\n",
      "iliar, softless care, he would imprisour\n",
      "When every time and my sadne much appointed:\n",
      "Thou kild'st, \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.154914001499089784350967407\n",
      "-----------------------------------------------------------------\n",
      "ingment: no more life of my son,\n",
      "And for lience in at all offence, look'd of ding;\n",
      "It is study in co\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.155777229736405536684417725\n",
      "-----------------------------------------------------------------\n",
      "ry doth dislikelg: Well,\n",
      "The tamidest throat women'st nothing, and\n",
      "promise Bianca find his butting o\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.152224275821753402280807495\n",
      "-----------------------------------------------------------------\n",
      "e, good fellow's now, and to-morrow.\n",
      "\n",
      "SEBASTIAN:\n",
      "Let them all things braved: when time of name.\n",
      "\n",
      "Boa\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1530599124461751491451263436\n",
      "-----------------------------------------------------------------\n",
      ", pale and suffer for your mointua;\n",
      "Command,-- you.\n",
      "\n",
      "ANTONIO:\n",
      "Ariel and ry is.\n",
      "\n",
      "GONZALO:\n",
      "Uninstructi\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1530468390200228997688293465\n",
      "-----------------------------------------------------------------\n",
      "or, that watch that\n",
      "Was not they will be withat then.\n",
      "\n",
      "ANTONIO:\n",
      "Tranio, thou art not to stubble.\n",
      "\n",
      "SE\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.149969933108450590241241455\n",
      "-----------------------------------------------------------------\n",
      "ll and hence.\n",
      "\n",
      "MIANANA:\n",
      "Well, go; then it is.\n",
      "\n",
      "ANTONIO:\n",
      "Why should I am, with his good behanning\n",
      "fol\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.151704612328077964485168457\n",
      "-----------------------------------------------------------------\n",
      "request.\n",
      "\n",
      "SEBASTIAN:\n",
      "In thine.\n",
      "\n",
      "ANTONIO:\n",
      "A queen of such daughter of my boomsemple!\n",
      "\n",
      "SEBASTIAN:\n",
      "I kn\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1537079108805652040971755981\n",
      "-----------------------------------------------------------------\n",
      ", I relent hence!\n",
      "\n",
      "SEBASTIAN:\n",
      "I live soft putting them. See what would\n",
      "Without contentions how mere \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1530912341499056390630722046\n",
      "-----------------------------------------------------------------\n",
      "her; thy crussing;\n",
      "Would not be franking now? accept not of our meaning,\n",
      "With ittentioners put a nup\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1525647533129098601123809814\n",
      "-----------------------------------------------------------------\n",
      "O:\n",
      "And arnoble crook to Natrow himself.\n",
      "\n",
      "MIRANDA:\n",
      "Padua, monstrous grandsire!\n",
      "\n",
      "ANTONIO:\n",
      "Or off that \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1517002327414594386594772339\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "FERDINAND:\n",
      "Than over one o'ersafel, sir.\n",
      "\n",
      "ANTONIO:\n",
      "That cold e-rest Ben, having raised the poornie\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1499334954364646247039794922\n",
      "-----------------------------------------------------------------\n",
      "oh you\n",
      "Wonder thyself, and wretch'd great brinegroshes,\n",
      "Womening and the own imagiral tale.\n",
      "We spoke\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1513632429770522766071319585\n",
      "-----------------------------------------------------------------\n",
      "nno\n",
      "Diein your must ask them.\n",
      "\n",
      "GONZALO:\n",
      "All thinks that to\n",
      "which preserved in your penalty,- Sittin;\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1497436343613499528007507324\n",
      "-----------------------------------------------------------------\n",
      "or he begun thy course; all soleuns.\n",
      "\n",
      "ALONSO:\n",
      "Why then thou misstanst.\n",
      "\n",
      "ANTONIO:\n",
      "What islution is in\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1497830585237153120632171632\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "How thou dost swear to Angelo! So fiver no\n",
      "good duty, for strength of night means drop but camble\n",
      "i\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1497822892204077100648880005\n",
      "-----------------------------------------------------------------\n",
      " Angelo\n",
      "As every goodly pagendages sure.\n",
      "\n",
      "ANTONIO:\n",
      "And what you are cut out touch him, as thou sensh\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1498350668175812059381484985\n",
      "-----------------------------------------------------------------\n",
      "pen of this instrument or'till not our business\n",
      "And till they godly wand at your merry.\n",
      "\n",
      "ALONSO:\n",
      "Why\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1467486351934193601247787476\n",
      "-----------------------------------------------------------------\n",
      ", jispited with widow\n",
      "A baby of the pilfuinetheight scarrable\n",
      "TothaTversy of koking son-xost.\n",
      "\n",
      "SEBAS\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1467596076300617246297836304\n",
      "-----------------------------------------------------------------\n",
      "is a\n",
      "kind of such an angry zenchery, Continence\n",
      "Marriers full; after awainkderious.--Hark, Baptory! \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1485106189732144918489456177\n",
      "-----------------------------------------------------------------\n",
      " laugh, and tell a piece of mide,\n",
      "Thou priest thus warm to run not fitted his\n",
      "father.\n",
      "\n",
      "ANTONIO:\n",
      "When\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1462291393121419835128784184\n",
      "-----------------------------------------------------------------\n",
      "thes removes alrouely forsward.\n",
      "\n",
      "GONZALO:\n",
      "All end, poor marshan, fine but manifold\n",
      "With your devil, \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1487176496010132715631484985\n",
      "-----------------------------------------------------------------\n",
      "han Dian's fooling-hinds shall: kindly\n",
      "In pincolity once more, so, his bears for thee and mother: an\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1463125120158004654722213745\n",
      "-----------------------------------------------------------------\n",
      "mouth of woathand\n",
      "No play'd foul purpose our, and bid my woo', sir,\n",
      "What,' good sweet sides, I mista\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1454005215872864709169387817\n",
      "-----------------------------------------------------------------\n",
      "he stirringo together,\n",
      "To visit. I have lessed in this charget o'ershapen.\n",
      "\n",
      "SEBASTIAN:\n",
      "To the of its\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1458312626121440149745941162\n",
      "-----------------------------------------------------------------\n",
      "3o Deep upon thee.\n",
      "\n",
      "ALONSO:\n",
      "Who is come? 'tis none.\n",
      "\n",
      "ASTON:\n",
      "It is this curse of death.\n",
      "\n",
      "SEBASTIAN:\n",
      "P\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1450836061309038147188186646\n",
      "-----------------------------------------------------------------\n",
      "uot sufferance, in hearing calls.\n",
      "\n",
      "SEBASTIAN:\n",
      "At the midnight all deliverance, being mine,--\n",
      "\n",
      "GONZAL\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1455784970993825258661270142\n",
      "-----------------------------------------------------------------\n",
      "e the caition's sorrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For thy late have dorper'd us of, buds, our wood,\n",
      "you say'st wrather I have s\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1473202598952161093090057373\n",
      "-----------------------------------------------------------------\n",
      " your prefliners,\n",
      "Who she shall ashabing done:\n",
      "His fortunes in this water than have,\n",
      "Methought her l\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1458075124993192547170639038\n",
      "-----------------------------------------------------------------\n",
      "r over-truncles? I will pry; ill liege\n",
      "The cunning mangle upon it to bride my,\n",
      "Than since from me.\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1407204679466734999612808228\n",
      "-----------------------------------------------------------------\n",
      "ar\n",
      "Attend us inclining of my foul princess.\n",
      "\n",
      "ANTONIO:\n",
      "You be so hither, wither.\n",
      "\n",
      "VOOS:\n",
      "These suwds o\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1440362315653343383943557748\n",
      "-----------------------------------------------------------------\n",
      "more hearts mike to his hown recreant,\n",
      "Which gentlewance was soleford.\n",
      "\n",
      "SEBASTIAN:\n",
      "You married, migh\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch loss: 1.1459151048099343015243530273\n",
      "-----------------------------------------------------------------\n",
      ".\n",
      "\n",
      "ALONSO:\n",
      "Is this! to answer?\n",
      "\n",
      "SEBASTIAN:\n",
      "Ungirning overleastor'd for Tranio,\n",
      "Disdock'd my fah with\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Epoch [125]/[200] loss: 1.0141407251358032\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-80e2c4f0ee50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtarget_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcharRnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-3cd0400d7de8>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, hidden_state)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0membedds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;31m#out = self.drop_out(out)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    659\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    662\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    663\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_train_loss = []\n",
    "loss = 0\n",
    "for i in range(epochs):\n",
    "    train_loss = 0\n",
    "    n=0\n",
    "    data_ptr = np.random.randint(100)\n",
    "    while True:\n",
    "        hidden_state = charRnn.initHidden(1)\n",
    "        input_sequence = dataset[data_ptr:data_ptr+seq_len]\n",
    "        target_sequence = dataset[data_ptr+1:data_ptr+seq_len+1]\n",
    "        \n",
    "        out, hidden_state = charRnn(input_sequence, hidden_state)\n",
    "        \n",
    "        loss = criterion(torch.squeeze(out), torch.squeeze(target_sequence))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        data_ptr += seq_len\n",
    "        n += 1\n",
    "        \n",
    "        print(f\"Epoch [{i+1}]/[{epochs}] loss: {loss.item()}\", end=\"\\r\", flush=True)\n",
    "        \n",
    "        if data_ptr + seq_len + 1 > data_size:\n",
    "            break\n",
    "    total_train_loss.append(train_loss/n)\n",
    "    print(f\"Epoch loss: {train_loss/n}\")\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    \n",
    "    hidden_state = charRnn.initHidden(1)\n",
    "    rand_index = np.random.randint(data_size-1)\n",
    "    input_sequence = dataset[rand_index : rand_index+1]\n",
    "    for i in range(100):\n",
    "        out, hidden_state = charRnn(input_sequence, hidden_state)\n",
    "        \n",
    "        # construct categorical distribution and sample a character\n",
    "        out = F.softmax(torch.squeeze(out), dim=0)\n",
    "        dist = Categorical(out)\n",
    "        index = dist.sample()\n",
    "        \n",
    "        print(dictionary[index.item()], end=\"\")\n",
    "        \n",
    "        input_sequence[0][0] = index.item()\n",
    "    print(\"\\n-----------------------------------------------------------------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    #=====================Priming====================\n",
    "    hidden_state = charRnn.initHidden(1)\n",
    "    input_sequence = []\n",
    "    print(\"Priming sequence: \", end=\"\")\n",
    "    for i in range(5):\n",
    "        index = np.random.randint(data_size-1)\n",
    "        char = dataset[index:index+1]\n",
    "        input_sequence.append(char[0])\n",
    "        print(dictionary[char[0][0]], end=\"\") #print the character\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    input_sequence = torch.tensor(input_sequence).to(device)\n",
    "    input_sequence = torch.unsqueeze(input_sequence, dim=1)\n",
    "    \n",
    "    out, hidden_state = charRnn(input_sequence, hidden_state)\n",
    "    \n",
    "    # construct categorical distribution and sample a character\n",
    "    out = F.softmax(torch.squeeze(out), dim=0)\n",
    "    dist = Categorical(out)\n",
    "    index = dist.sample()\n",
    "    #================================================\n",
    "    \n",
    "    input_sequence = input_sequence[0:1]\n",
    "    input_sequence[0][0] = index[-1]\n",
    "    \n",
    "    for i in range(100):\n",
    "        out, hidden = charRnn(input_sequence, hidden_state)\n",
    "        \n",
    "        # construct categorical distribution and sample a character\n",
    "        out = F.softmax(torch.squeeze(out), dim=0)\n",
    "        dist = Categorical(out)\n",
    "        index = dist.sample()\n",
    "        \n",
    "        print(dictionary[index.item()], end=\"\")\n",
    "        input_sequence[0][0] = index.item()\n",
    "    print(\"\\n-----------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primers = [\"The\", \"What is\", \"Shall I give\", \"XhNYB BHN BYFVuhsdbs\"]\n",
    "\n",
    "for word in primers:\n",
    "    #=====================Priming====================\n",
    "    hidden_state = charRnn.initHidden(1)\n",
    "    input_sequence = []\n",
    "    print(\"Priming sequence: \", end=\"\")\n",
    "    for c in word:\n",
    "        ind = dictionary.index(c)\n",
    "        input_sequence.append([ind])\n",
    "        print(c, end=\"\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    input_sequence = torch.tensor(input_sequence).to(device)\n",
    "    \n",
    "    out, hidden_state = charRnn(input_sequence, hidden_state)\n",
    "    \n",
    "    # construct categorical distribution and sample a character\n",
    "    out = F.softmax(torch.squeeze(out), dim=0)\n",
    "    dist = Categorical(out)\n",
    "    index = dist.sample()\n",
    "    #================================================\n",
    "    input_sequence = input_sequence[0:1]\n",
    "    input_sequence[0][0] = index[-1]\n",
    "    \n",
    "    for i in range(100):\n",
    "        out, hidden = charRnn(input_sequence, hidden_state)\n",
    "        \n",
    "        # construct categorical distribution and sample a character\n",
    "        out = F.softmax(torch.squeeze(out), dim=0)\n",
    "        dist = Categorical(out)\n",
    "        index = dist.sample()\n",
    "        \n",
    "        print(dictionary[index.item()], end=\"\")\n",
    "        input_sequence[0][0] = index.item()\n",
    "    print(\"\\n-----------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), total_train_loss)\n",
    "#plt.plot(range(epochs), total_valid_loss)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper right')\n",
    "plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25, wspace=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.tensor(total_train_loss)\n",
    "exp_loss = torch.exp(l)\n",
    "print(exp_loss, l)\n",
    "plt.plot(exp_loss)\n",
    "plt.title('Model perplexity')\n",
    "plt.ylabel('perplexity')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnEklEQVR4nO3deXxcdb3/8ddnsq9t0ibp3pS0dKEtLaRUBWxB9h1RAZGLXK4oPxW8ev2Jy1X0eq9eFRfk/lQUFBBZFFAsyCKylb0bobXQfUvTJt2TNkmzfH5/zElvTJM2TTM5s7yfj8cwM2fOnPnkPMp7zny/53y/5u6IiEjqiIRdgIiIDCwFv4hIilHwi4ikGAW/iEiKUfCLiKQYBb+ISIpR8EtSMbNyM3MzS+/Fuh83s/kDUdfhmNktZvbbftjOMjObe/QVSTJT8EtozGydme03s6Fdli8Jwrs8pNISlrsf5+7PQ/99mUjyUfBL2NYCV3Y8MbNpQE545fS/3vz6EBlICn4J273AP3V6fg1wT+cVzGyQmd1jZnVmtt7MvmZmkeC1NDP7gZltM7M1wPndvPdOM6sxs2oz+7aZpR2uqE5NRteb2ebg/V/o9HrEzG42s9Vmtt3MHjKz4i7vvc7MNgB/O9z2uvn895jZK2a2y8ze6mi+MbP3BX/r6OD58cE6k4Ln68zsDDM7B/gKcLmZNQTb+LCZLezyOV8wsz8ebn9IclHwS9heAwrNbHIQyJcDXZsnfgoMAo4B5hD9org2eO0TwAXATKAS+FCX994NtALjg3XOAv7lCOo7DZgQvO9mMzsjWH4jcElQzwhgJ/A/Xd47B5gMnN2L7R1gZiOBx4FvA8XAvwEPm1mJu78C/AK428xyiH5xfs3d3+m8DXd/Evgv4EF3z3f344HHgHFmNrnTqh8LtiEpRMEv8aDjqP9M4B2guuOFTl8GX3b3endfB9wKXB2s8hHgx+6+0d13AN/p9N4y4Fzgc+6+191rgR8BVxxBbd8M3vs28Gv+t1nqk8BX3X2TuzcDtwAf6tKsc0vw3sZebK+zjwFPuPsT7t7u7s8AC4DzOrZL9IvwDWAzB3/hdCuo88Fg+5jZcUA5MK8375fkobZHiQf3Ai8C4+jSzAMMBTKB9Z2WrQdGBo9HABu7vNZhLJAB1JhZx7JIl/UPp+u2p3Xa9qNm1t7p9TagrIf3Hm57nY0FPmxmF3ZalgE8B+DuLWb2G+A24PN+ZCMt3g3cb2ZfI/rl+VDwhSApREf8Ejp3X0+0k/c84JEuL28DWoiGYYcx/O+vghpgdJfXOmwEmoGh7j44uBW6+3FHUF7XbW/utO1zO213sLtnu3t1p/W7C+SettfZRuDeLtvOc/fvwoGmoG8Q/cVwq5ll9VD7QZ/v7q8B+4FTgY+iZp6UpOCXeHEdcLq77+280N3bgIeA/zSzAjMbC3ye/+0HeAi40cxGmVkRcHOn99YATxMNx8KgQ7bCzOYcQV3/bma5QbPItUSbSgB+HtQ0FsDMSszs4qPYXme/BS40s7ODzutsM5sb/I0G/Aa4k+g+qwH+o4fP2gqUd3SEd3IPcDvQ6u5xcR2DDCwFv8QFd1/t7gt6ePmzwF5gDTAf+B1wV/DaL4GngLeARRz8i+GfiDYV/Z1oB+wfgOFHUNoLwCrgWeAH7v50sPwnRDtLnzazeqKd1LOPYnsHuPtG4GKiZ+XUEf0F8EWi/7/eSLQ56d+DJp5rgWvN7NRuPuv3wf12M1vUafm9wFR0tJ+yTBOxiBwsuHhsLZDh7q3xtr2jrCUHqAVOcPeVYdYi4dARv0jquQF4U6GfunRWj0gKMbN1gBG9BkFSlJp6RERSjJp6RERSTEI09QwdOtTLy8vDLkNEJKEsXLhwm7uXdF2eEMFfXl7OggU9neknIiLdMbP13S1XU4+ISIpR8IuIpBgFv4hIilHwi4ikGAW/iEiKUfCLiKQYBb+ISIpJ6uB/7t1a7py/lprdjYdfWUQkRSTEBVx99dw7tdzz6nr+Y97fmVVexAXTR3DutGGUFmSHXZqISGgSYpC2yspK7+uVu6vrGni8qobHq2p4d2s9ZjB7XHH0S2DqMIbk9zRrnYhIYjOzhe5eedDyZA/+zlZsrWdeVQ3zqjazpm4vaRFjxOBsDMMsOlZtx6Tc1vEfj05c6u7BPbS707Hb0tOM9IiRkRYJHkfICO6L8jL42OyxvLdiyIHtiogMFAV/J+7OO1vqebyqhupdjf8Q6vCPQd/xZRC9h0jwmCDH29qd1janpa2d1vbgvs1pbW9n7bZ9bGtoZlZ5ETd+YAKnjB+qLwARGTA9BX9St/H3xMyYPLyQycMLY/o5TS1t/H7BRv7f86u5+s43mDF6MDd9YAJzJ5boC0BEQpOSR/wDrbm1jYcXVvM/z62ielcj00YO4sYPTOCMyaX6AhCRmOnpiD+pT+eMF1npaXx09hie/+JcvnfZdHY3tvCJexZwxR2v0dLWHnZ5IpJiFPwDKCMtwkdmjeZvX5jD186fzOtrd3Dvq90Oly0iEjMK/hCkp0W47pRxzDm2hB89s4La+qawSxKRFKLgD4mZ8Y0Lp9DU2sZ3//JO2OWISApR8IfomJJ8PnHqMTyyqJoF63aEXY6IpAgFf8g+c/p4hg/K5t//tIy29vg/w0pEEp+CP2S5mel87fwpLK/Zw32vq6NXRGJPwR8Hzps2jJPHD+EHT73L9obmsMsRkSSn4I8DZsY3LzqOffvb+N6T74ZdjogkOQV/nBhfWsA/nzKOBxdsZPGGnWGXIyJJTMEfR278wATKCrP4ujp6RSSGFPxxJD8rna+cN5m3q3fzwJsbwi5HRJKUgj/OXHT8CGaPK+b7T73Lzr37wy5HRJKQgj/OmBnfungq9U2t/OTZlWGXIyJJSMEfhyYOK+C0iSW8tLIu7FJEJAkp+OPUhLIC1m/fp2GbRaTfKfjj1PiSfFrbnQ079oVdiogkmZgFv5mNNrPnzGy5mS0zs5uC5beYWbWZLQlu58WqhkRWUZoPwKrahpArEZFkE8s5d1uBL7j7IjMrABaa2TPBaz9y9x/E8LMT3jEleQCsrlPwi0j/ilnwu3sNUBM8rjez5cDIWH1esinMzqCsMEtH/CLS7wakjd/MyoGZwOvBos+YWZWZ3WVmRT2853ozW2BmC+rqUvPsloqSfFbX7Q27DBFJMjEPfjPLBx4GPufue4CfARXADKK/CG7t7n3ufoe7V7p7ZUlJSazLjEvjS/NZU9uAu4ZvEJH+E9PgN7MMoqF/n7s/AuDuW929zd3bgV8CJ8WyhkRWUZJPfXMrtfUaqllE+k8sz+ox4E5gubv/sNPy4Z1WuxRYGqsaEt344Mye1WrnF5F+FMuzek4GrgbeNrMlwbKvAFea2QzAgXXAJ2NYQ0KrKAmCv66B940fGnI1IpIsYnlWz3zAunnpiVh9ZrIpK8wiPytdZ/aISL/SlbtxzMyoKMnTmT0i0q8U/HGuoiRfR/wi0q8U/HGuojSfLXuaaGhuDbsUEUkSCv4419HBu0ZDN4hIP1Hwx7nxpdExe9TcIyL9RcEf58YOySM9YhqsTUT6jYI/zmWkRRgzJFdH/CLSbxT8CWC8BmsTkX6k4E8AFaX5rN++V9Mwiki/UPAngPEl+bS0aRpGEekfCv4EUKHB2kSkHyn4E0DHNIyrdGaPiPQDBX8C6JiGcXWtOnhF5Ogp+BNEdBpGHfGLyNFT8CeI8aX5rNY0jCLSDxT8CaJjGsY6TcMoIkdJwZ8gOgZr0xW8InK0FPwJ4sD8u2rnF5GjpOBPEB3TMGroBhE5Wgr+BNExDaOaekTkaCn4E4hO6RSR/qDgTyAVpfnU7NY0jCJydBT8CUTTMIpIf1DwJ5COaRjV3CMiR0PBn0A6pmFUB6+IHA0FfwLpmIZRg7WJyNFQ8CeY8SX5Gp5ZRI6Kgj/BaBpGETlaCv4EUxFMw7hR0zCKSB8p+BNMx5g96uAVkb5S8CeYjmkYNWaPiPSVgj/BdEzDqCN+EekrBX8C0pg9InI0FPwJaHxpNPg1DaOI9IWCPwFVlORT36RpGEWkbxT8CejANIxq7hGRPohZ8JvZaDN7zsyWm9kyM7spWF5sZs+Y2crgvihWNSSrA9MwqoNXRPoglkf8rcAX3H0y8B7g02Y2BbgZeNbdJwDPBs/lCJQVZjEkL5MHF2ykqaUt7HJEJMHELPjdvcbdFwWP64HlwEjgYuDuYLW7gUtiVUOyMjP++7LpLK3ew1ceeVudvCJyRAakjd/MyoGZwOtAmbvXQPTLASjt4T3Xm9kCM1tQV1c3EGUmlDOmlPGvZxzLI4ur+fXL68IuR0QSSMyD38zygYeBz7n7nt6+z93vcPdKd68sKSmJXYEJ7LOnj+esKWX85xPLeWX1trDLEZEEEdPgN7MMoqF/n7s/EizeambDg9eHA7WxrCGZRSLGDy+fwbiheXz6vkUauE1EeiWWZ/UYcCew3N1/2Omlx4BrgsfXAH+KVQ2pID8rnTuuPpHWdueT9y6kcb86e0Xk0GJ5xH8ycDVwupktCW7nAd8FzjSzlcCZwXM5CseU5HPbFTNZvmUPNz9Spc5eETmk9Fht2N3nA9bDyx+I1eemqtMmlfJvZ03k+0+9y9QRg/jE+48JuyQRiVO9OuI3s4fN7Hwz05W+cez/zK3g3KnD+M5flvPSSp0JJSLd622Q/wz4KLDSzL5rZpNiWJP0kZnxgw8fz4TSAj57/2KqdzWGXZKIxKFeBb+7/9XdrwJOANYBz5jZK2Z2bXDmjsSJvKx0fnH1iexpbOF3r68PuxwRiUO9broxsyHAx4F/ARYDPyH6RfBMTCqTPisfmsf7KoYyr6pGHb0icpDetvE/ArwE5AIXuvtF7v6gu38WyI9lgdI3F0wfzvrt+1ha3etr5kQkRfT2iP9X7j7F3b/TMdyCmWUBuHtlzKqTPjtn6jDSI8a8qs1hlyIicaa3wf/tbpa92p+FSP8anJvJKRPU3CMiBztk8JvZMDM7Ecgxs5lmdkJwm0u02Ufi2AXTR1C9q5ElG3eFXYqIxJHDXcB1NtEO3VFA52EX6oGvxKgm6SdnTikjMy3CvKoaZo7RfDciEnXI4Hf3u4G7zewyd394gGqSfjIoJ4P3HzuUx6tq+Op5k4lEerqQWkRSySGD38w+5u6/BcrN7PNdX+8y+JrEoQumj+Cvy2tZuGEns8qLwy5HROLA4Tp384L7fKCgm5vEuTOmlJGVHmHeWzq7R0SiDtfU84vg/ptdXzOzzFgVJf0nPyud0yaW8sTSLXz9wuNIU3OPSMrr7QVczwfTJ3Y8nwW8GauipH9dcPxw6uqbeX3t9rBLEZE40Nthmb8DPGlmtxGdMP1c4NqYVSX96vRJpeRkpDGvqob3VQwNuxwRCVlvB2l7CvgU0fF5/hk4z90XxbIw6T+5mel8YHIpTy7dQmtbe9jliEjIetvU8+/AT4H3A7cAz5vZ+TGsS/rZBdNHsGPvfl5do+YekVTX2yEbhgInufurQYfv2cDnYlaV9Lu5E0vIz0pn3ls1YZciIiHrbVPPTQBmNjF4vt7dz4xlYdK/sjPSOHNKGU8u28L+VjX3iKSy3jb1XAgsAZ4Mns8ws8diWJfEwPnThrO7sYWXV20LuxQRCVFvm3puAU4CdgG4+xJgXEwqkpg59dihFGSn82cN1SyS0nob/K3uvrvLMo31m2Cy0tM4+7hhPLNsK00tbWGXIyIh6W3wLzWzjwJpZjbBzH4KvBLDuiRGLpg+nPrmVl5cURd2KSISkt4G/2eB44Bm4H5gDzqrJyGdPH4oRbkZPP62zu4RSVW9unLX3fcBXw1uksAy0iKcM3UYjy3ZTOP+NnIy08IuSUQG2OGGZf4zh2jLd/eL+r0iibmLjh/J/W9s5KllW7hk5siwyxGRAXa4I/4fDEgVMqBmjytmTHEuD7y5QcEvkoIONyzzCx2Pg2GYJxH9BfCuu++PcW0SI5GIcfms0Xz/qXdZt20v5UPzDv8mEUkavb2A63xgNXAbcDuwyszOjWVhEluXnTCKiMFDCzaGXYqIDLDentVzK3Cau8919znAacCPYleWxNqwQdmcNrGUPyzcpBE7RVJMb4O/1t1XdXq+BqiNQT0ygD4yazS19c28oHP6RVJKb4N/mZk9YWYfN7NrgD8Db5rZB83sgzGsT2Lo9EmlDM3P4oE31dwjkkp6G/zZwFZgDjAXqAOKgQuBC2JSmcRcRlqEy04cyd/eqaW2vinsckRkgBz2Ai4zSwOq3F1t+knoI5Wj+cULa3h4YTU3zK0IuxwRGQCHPeJ39zZAF2olqYqSfE4qL+ahBRtx17h7Iqmgt009r5jZ7WZ2qpmd0HGLaWUyYD4yazRrt+3ljbU7wi5FRAZAb4P/fUQHafsW0VM7b+UwV/Wa2V1mVmtmSzstu8XMqs1sSXA7r6+FS/85b9owCrLSeVDn9IukhN4O0nZaH7b9G6IXe93TZfmP3F1DQcSR3Mx0LpwxgkcWbeKWi46jMDsj7JJEJIZ6e+VumZndaWZ/CZ5PMbPrDvUed38RUNtBgrhi1miaWtp5bIlm5xJJdr1t6vkN8BQwIni+gr6Px/8ZM6sKmoKKelrJzK43swVmtqCuThcYxdq0kYOYNKyAB3VOv0jS623wD3X3h4B2AHdvBfoyd9/PgApgBlBDtK+gW+5+h7tXuntlSUlJHz5KjoSZccWs0bxdvZtlm7vOsikiyaS3wb/XzIYQjM1vZu8Bjjgd3H2ru7e5ezvwS6ITuEucuGTmSDLTIzyko36RpNbb4P888BhwjJm9TLTD9rNH+mFmNrzT00uBpT2tKwNvcG4m5xw3jEcXV2sydpEk1tvg/zvwKPAm0aEbfkm0nb9HZnY/8Cow0cw2BZ3B3zOzt82siugIn//a58olJi6fNZo9Ta08tWxL2KWISIz06nROokf4e4D/Cp5fCdwLfLinN7j7ld0svvOIqpMB995jhjC6OIcH3tjIxTM0O5dIMurtEf9Ed/8Xd38uuF0PHBvLwiQckYhx1eyxvLpmOy+v2hZ2OSISA70N/sVBhy4AZjYbeDk2JUnYPv6+csYU5/L1Py1lf6smaRFJNr0N/tlEx+tZZ2briLbdz+nUXi9JJDsjjVsumsLqur38+uW1YZcjIv2st23858S0Cok7p08q44zJpfzk2ZVcNGMEwwflhF2SiPSTXh3xu/v6Q91iXaSE4xsXHkdbu/Ofjy8PuxQR6Ue9beqRFDS6OJcb5lYwr6pGHb0iSUTBL4f0qTkVjCnO5RuPLVNHr0iSUPDLIXV09K6qbVBHr0iSUPDLYXXu6K3Z3Rh2OSJylBT80ivq6BVJHgp+6ZXOHb2vqKNXJKEp+KXXOjp6v66OXpGEpuCXXuvc0XuXOnpFEpaCX47I6ZPKOGtKGT98egVLqzVTl0giUvDLEfvuZdMpzsvk079bRH1TS9jliMgRUvDLESvOy+T2j85k085Gbn7kbdw97JJE5Ago+KVPKsuL+eLZE3m8qobfvqbhmkQSiYJf+uz6U4/htIkl/Me85WrvF0kgCn7ps0jEuPUjMxiSH23v36P2fpGEoOCXo1Kcl8lPr4y293/5YbX3iyQCBb8ctQPt/W/XcK/a+0XinoJf+kVHe/+35y3n7U1q7xeJZwp+6Rdq7xdJHAp+6Tcd5/dX72rkkttf5ullW9TmLxKHFPzSr04cW8yvPz6LSMS4/t6FXHHHa1Rt2hV2WSLSiYJf+t37jy3hyZtO5duXTGVVbQMX3f4yn3tgMZt27gu7NBEBLBF+ildWVvqCBQvCLkP6oL6phZ+/sJpfvbQWB647ZRw3zK2gMDsj7NJEkp6ZLXT3yq7LdcQvMVWQncEXz57E3/5tLhdMG87Pnl/N3O8/z+NVNWGXJpKyFPwyIEYOzuGHl8/gz585hdHFuXz6d4v48iNVNO5vC7s0kZSj4JcBNW3UIP7wqfdyw9wK7n9jIxfdPp93t9SHXZZISlHwy4DLSIvwpXMmce91J7FzXwsX3T6f+15fr1M/RQaIgl9Cc+qEEv5y06mcNK6Yrz66lP9z3yJ279OFXyKxpuCXUJUUZHH3tSdx87mTeObvWznvtpdYuH5H2GWJJDUFv4QuEjE+NaeC33/qvUQi8OGfv8qn71ukMX9EYkTBL3Fj5pgiHr/xVK5/fwUvrqjjwtvnc9WvXuPFFXVq/xfpR7qAS+JSfVMLv3t9A3fOX0ttfTNThhfyyTnHcP604aSn6XhFpDcG/AIuM7vLzGrNbGmnZcVm9oyZrQzui2L1+ZLYCrIz+OScCl760mn892XTaGpt46YHlnDarc9z3+vraWuP/wMWkXgVy0On3wDndFl2M/Csu08Ang2ei/QoKz2Ny2eN4a//OodfXH0iQ/Oz+OqjS7nsZ6+wYqvO/xfpi5gFv7u/CHQ9PeNi4O7g8d3AJbH6fEkukYhx9nHDeOSG9/Hjy2ewfvtezr/tJX70zAqaW3X1r8iRGOjG0jJ3rwEI7kt7WtHMrjezBWa2oK6ubsAKlPhmZlwycyR//fwczps2nJ88u5ILbpvPog07wy5NJGHEbS+Zu9/h7pXuXllSUhJ2ORJnhuRn8ZMrZnLXxyvZ29zKZT97hVseW8be5tawSxOJewMd/FvNbDhAcF87wJ8vSeb0SWU8/fk5XP2esdz96jrO+tGLPLVsC+3q/BXp0UAH/2PANcHja4A/DfDnSxLKz0rnWxdP5feffC/ZGRE+ee9Czv7xi/x+wUb2t7aHXZ5I3InZefxmdj8wFxgKbAW+AfwReAgYA2wAPuzuh70+X+fxS2+1tLXzeFUNP39hNe9sqaesMIt/PnkcV84eo8lfJOX0dB6/LuCSpOTuvLRyG794cTUvr9pOQVY6H509hmtPHsewQdlhlycyIBT8krKWVu/mFy+u4fGqzaRFjPOnDeeq94ylcmwRZhZ2eSIxo+CXlLdxxz7unL+Whxduor65lYllBVz1njFcMnOkmoEkKSn4RQL79rcy760afvv6eqo27SYnI42LZ4zgqtljmTZqUNjlifQbBb9IN6o27eJ3r2/gT0s209jSxvRRg7hq9hguPH4EuZnpYZcnclQU/CKHsKephT8urua3r61nxdYGCrLTueyEUXx09hiOLSsIuzyRPlHwi/SCu7Ng/U7ue209T7y9hf1t7ZxUXsxV7xnDOVOHkZWeFnaJIr2m4Bc5Qjv27ucPCzdy3+sbWL99H8V5mXzoxFF86MRR+hUgCUHBL9JH7e3Oy6u3cd9rG3hm+Vba2p0pwwv54Akjuej4EZQW6roAiU8KfpF+sK2hmT+/tZk/Lq7mrU27iRicPH4ol84cydnHDSMvSx3CEj8U/CL9bHVdA39cXM2ji6vZtLORnIw0zpxSxjlThzHn2BJ9CUjoFPwiMeLuLFy/k0cXV/PE2zXs3NdCVnqEUycM5azjhnHG5DKK8zLDLlNSkIJfZAC0trXz5rqdPLVsC08v28Lm3U1EDE4aV8zZxw3jxLFF5Gamk5OZRk5GGrmZaWSlRzR0hMSEgl9kgLk7S6v38NSyLTy1bAsraxt6XDcnI42czDSmjRzEB08YyVlThpGTqVNH5ego+EVCtqaugVW1DTS2tNHU0kbj/jYaW9oPPK9vauHFFduo3tVIflY6508bzgdPGMms8mIiEf0ikCPXU/Cr90lkgBxTks8xJfmHXKe93Xlt7XYeWVTNn6s28+CCjYwuzuHSmaO47ISRjB2SN0DVSjLTEb9InNq3v5Wnlm3h4YXVvLx6G+5QUpDF+JJ8xpfmU1GSx/jSAsaX5lNWmKV+AjmIjvhFEkxuZjqXzhzFpTNHUbO7kceranhnSz2raqOnkdZ3mlg+Pyud8aX5zD6mmFPHl1BZXkR2hvoIpHs64hdJQO5OXX0zq+oaWF23l9W1Dfx98x4Wb9xJS5uTlR7hpHHFnDJ+KKdMGMrkYYXqJ0hB6twVSQF7m1t5Y+0OXlq5jfmr6lixNXom0ZC8TN5bMYQTxhQxc8xgpowo1IBzKUBNPSIpIC8rndMmlXLapFIAtu5pYv7KbcxftY3X1mxnXlUNAJlpEaaMKGTmmMHMHFPEzNGDGVWUo36CFKEjfpEUsmV3E0s27mTxhl0s3riLqk27aGppB6AgK52RRTmMGJzDiMHZjBycG9znMLIoh7KCbDUXJRgd8YsIwwZlc86g4ZwzdTgALW3tvLulnsUbd7Fqaz3Vu5rYvKuRhet3srux5R/em50RYUJpARPK8jm2rIBjy/KZUFrAyME5+kJIMAp+kRSWkRZh6shBTB158FzDDc2t1OxqpDq4ranby4qt9by8ahuPLKo+sF5eZhrjywoYNTiH0sIsSguyKSvMoqwwel9amE1BVrqakeKIgl9EupWflc6EsgImdDPpzO59LaysrefdrfWs3NrAytp6lm/ZwwsrmmnodJpph5yMNEYX5zCmOI+xQ3IZOySX0cW5jC3OZVRRLpnpkYH4kySg4BeRIzYoN4PK8mIqy4sPeq2huZXaPU1s3dNMbX0TtXuaqdndxIYd+9iwYy/zV9Ud6FcAiBiMKspl+qhBzBg9mONHD2bqiEEaqyiGFPwi0q/ys9LJP8TwFB3XIKzfsY/12/exYfteVtY2sHjDrgNnHaVFjIllBRw/ejAzRg9iQlkBg3MyGJybSWF2Oulp+oVwNBT8IjKgzIzSwmxKC7OZ1eUXQ219E1Ubd/PWpl0s2biLx6s2c/8bGw7aRkFWOoNyMxicm8GgnAwKsjLIzUwjNyuN3Mz06OPM6OO8rDTGFOcxcVgB+ZocB1Dwi0gcKS3I5owp2ZwxpQyIDlq3dvteNmzfx+7GFnbt28+uxhZ2N7awe18Lu4Jl2+r3s3d/K43729i7v/UfmpI6G1Ocy6RhBUweXsjk4QVMGlbImOJcdje2sGlnI5t27qN6V2PwOPp83/42po8aROXYIirLi5k8vJC0BD+LScEvInErEjEqSvKpOMyopl21tTuNLW3sa26lvrmVNXV7eadmD+9siXZC/3X5VtqDS5gixoHHHfKz0hlVlMOoohwy0yMsXL/zQDNUflY6M8cMpnJsMbPKixgzJJe9zW00NLdQ39RKQ3MrDcF9fVMr+VnpjBmSS/mQPMYU58ZF34WCX0SSTlrEon0NWemUAhUl+ZwZ/IoAaNzfxsraet6pqWft9r0Mzc9iVFEOIwfnMLool8Kcg08/rd7VyIJ1O1iwbidvrtvBj59dQV+ufx1WmM3Y4Itg7NBcSguyKcrNoCgvk6LcTIpzMynITo/ptRG6cldEpA92N7aweMNOtu5pIj8rg/zsdAqy0ynISic/O/qlk5eZTn1TK+t37GXd9n2s3xbcb4/eb2to7nbbEYOi3EwG52bwX5dOY/YxQ/pUo67cFRHpR4NyMpg7sfTw6+VmMD13MNNHDT7otb3NrWxv2M+OffvZuW8/O/fuZ+e+luA+eivMyej32hX8IiIhyctKJy/oAxhIOhlWRCTFKPhFRFKMgl9EJMWE0sZvZuuAeqANaO2u11lERGIjzM7d09x9W4ifLyKSktTUIyKSYsIKfgeeNrOFZnZ9dyuY2fVmtsDMFtTV1Q1weSIiySus4D/Z3U8AzgU+bWbv77qCu9/h7pXuXllSUjLwFYqIJKnQh2wws1uABnf/wSHWqQPW9/EjhgLx2peg2vpGtfWNauubRK5trLsfdOQ84J27ZpYHRNy9Pnh8FvCtQ72nu8KP4PMWxOtZQ6qtb1Rb36i2vknG2sI4q6cMeDQY+S4d+J27PxlCHSIiKWnAg9/d1wDHD/TniohIVCqcznlH2AUcgmrrG9XWN6qtb5KuttA7d0VEZGClwhG/iIh0ouAXEUkxSR38ZnaOmb1rZqvM7Oaw6+nMzNaZ2dtmtsTMQp1X0szuMrNaM1vaaVmxmT1jZiuD+6I4qu0WM6sO9t0SMzsvpNpGm9lzZrbczJaZ2U3B8tD33SFqC33fmVm2mb1hZm8FtX0zWB4P+62n2kLfb0EdaWa22MzmBc/7tM+Sto3fzNKAFcCZwCbgTeBKd/97qIUFghFKK+NhoLrgyukG4B53nxos+x6ww92/G3xpFrn7l+Kktls4zEV/A1TbcGC4uy8yswJgIXAJ8HFC3neHqO0jhLzvLHoud567N5hZBjAfuAn4IOHvt55qO4f4+Df3eaASKHT3C/r6/2kyH/GfBKxy9zXuvh94ALg45Jrikru/COzosvhi4O7g8d1EQ2PA9VBbXHD3GndfFDyuB5YDI4mDfXeI2kLnUQ3B04zg5sTHfuupttCZ2SjgfOBXnRb3aZ8lc/CPBDZ2er6JOPmHHzjsQHUhK3P3GoiGCHD4WaUH1mfMrCpoCgqlGaozMysHZgKvE2f7rkttEAf7LmiyWALUAs+4e9zstx5qg/D324+B/wu0d1rWp32WzMFv3SyLi2/uwGEHqpMe/QyoAGYANcCtYRZjZvnAw8Dn3H1PmLV01U1tcbHv3L3N3WcAo4CTzGxqGHV0p4faQt1vZnYBUOvuC/tje8kc/JuA0Z2ejwI2h1TLQdx9c3BfCzxKtGkqnmwN2ok72otrQ67nAHffGvzP2Q78khD3XdAO/DBwn7s/EiyOi33XXW3xtO+CenYBzxNtQ4+L/dahc21xsN9OBi4K+gYfAE43s9/Sx32WzMH/JjDBzMaZWSZwBfBYyDUB0YHqgg63jkHrzgKWHvpdA+4x4Jrg8TXAn0Ks5R90/EMPXEpI+y7oCLwTWO7uP+z0Uuj7rqfa4mHfmVmJmQ0OHucAZwDvEB/7rdvawt5v7v5ldx/l7uVEs+xv7v4x+rrP3D1pb8B5RM/sWQ18Nex6OtV1DPBWcFsWdm3A/UR/vrYQ/aV0HTAEeBZYGdwXx1Ft9wJvA1XBP/zhIdV2CtHmwypgSXA7Lx723SFqC33fAdOBxUENS4GvB8vjYb/1VFvo+61TjXOBeUezz5L2dE4REeleMjf1iIhINxT8IiIpRsEvIpJiFPwiIilGwS8ikmIU/CKAmbV1GnlxifXjaK5mVm6dRhcVCVsYk62LxKNGj16mL5L0dMQvcggWnTfhv4Mx2t8ws/HB8rFm9mwwaNezZjYmWF5mZo8G47m/ZWbvCzaVZma/DMZ4fzq4KlQkFAp+kaicLk09l3d6bY+7nwTcTnSERILH97j7dOA+4LZg+W3AC+5+PHAC0SuzASYA/+PuxwG7gMti+teIHIKu3BUBzKzB3fO7Wb4OON3d1wSDnm1x9yFmto3oZfstwfIadx9qZnXAKHdv7rSNcqLD+04Inn8JyHD3bw/AnyZyEB3xixye9/C4p3W609zpcRvqX5MQKfhFDu/yTvevBo9fITpKIsBVRKfog+hAWTfAgQk9CgeqSJHe0lGHSFROMOtShyfdveOUziwze53ogdKVwbIbgbvM7ItAHXBtsPwm4A4zu47okf0NREcXFYkbauMXOYSgjb/S3beFXYtIf1FTj4hIitERv4hIitERv4hIilHwi4ikGAW/iEiKUfCLiKQYBb+ISIr5/w6CNeaf+ykAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = history.history[\"val_loss\"]\n",
    "exp_loss = tf.exp(l)\n",
    "plt.plot(exp_loss)\n",
    "plt.title('Model perplexity')\n",
    "plt.ylabel('perplexity')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['First', 'Citizen:', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', 'All:', 'Speak,', 'speak.', 'First', 'Citizen:']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/shakespear_dataset.txt\", \"r\")\n",
    "text = f.read()\n",
    "wordlist = []\n",
    "wordlist = text.split()\n",
    "dictionary = []\n",
    "for i in wordlist:\n",
    "    if i not in dictionary:\n",
    "        dictionary.append(i)\n",
    "#print(dictionary)\n",
    "\n",
    "chunk_len = 15\n",
    "print()\n",
    "dataset = []\n",
    "k = 0\n",
    "for i in range(math.floor(len(wordlist)/chunk_len)):\n",
    "    sentence = []\n",
    "    for j in range(k,k+chunk_len):\n",
    "        sentence.append(wordlist[j])\n",
    "    k += chunk_len\n",
    "    dataset.append(sentence)\n",
    "#dataset = text.split()\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def indexing(sentence):\n",
    "#     indexed = []\n",
    "#     for word in sentence:\n",
    "#         ind = dictionary.index(word)\n",
    "#         indexed.append(ind)\n",
    "#     return indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset = []\n",
    "# for sentence in dataset:\n",
    "#     indexed_sentence = indexing(sentence)\n",
    "#     new_dataset.append(indexed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bow_embedder(sentence, vocab_len):\n",
    "#     bow = np.zeros(vocab_len)\n",
    "#     for index in sentence:\n",
    "#         bow[index] += 1\n",
    "#     return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(new_dataset)):\n",
    "#     sentence = new_dataset[i]\n",
    "#     bow = bow_embedder(sentence, len(dictionary))\n",
    "#     new_dataset[i] = bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for sentence in dataset:\n",
    "    x.append(np.array(sentence[:-1]))\n",
    "    y.append(np.array(sentence[1:]))\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, train_size=0.6, random_state=None, shuffle=False, stratify=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          1642816   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 300)         329400    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 200)         400800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 25669)       5159469   \n",
      "=================================================================\n",
      "Total params: 7,532,485\n",
      "Trainable params: 7,532,485\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn = keras.Sequential()\n",
    "#rnn.add(keras.layers.Input(shape=(None, len(dictionary))))\n",
    "rnn.add(keras.layers.Embedding(input_dim=len(dictionary), output_dim=64))\n",
    "rnn.add(keras.layers.GRU(300, return_sequences=True))\n",
    "rnn.add(keras.layers.LSTM(200, return_sequences=True))\n",
    "rnn.add(keras.layers.Dense(len(dictionary), activation=keras.activations.softmax))\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:748 train_step\n        loss = self.compiled_loss(\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 14) and (None, 14, 25669) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b8b064a6a473>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m             metrics=['accuracy'])\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:748 train_step\n        loss = self.compiled_loss(\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 14) and (None, 14, 25669) are incompatible\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "rnn.compile(optimizer=opt, \n",
    "            loss=loss_fn,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "history = rnn.fit(x=x_train, y=y_train, batch_size=100, epochs=40, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, input_size)\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        embedding = self.embedding(input_seq)\n",
    "        print(embedding.size())\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Data has 1115393 characters, 64 unique\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "########### Hyperparameters ###########\n",
    "hidden_size = 512   # size of hidden state\n",
    "seq_len = 100       # length of LSTM sequence\n",
    "num_layers = 3      # num of layers in LSTM layer stack\n",
    "lr = 0.002          # learning rate\n",
    "epochs = 100        # max number of epochs\n",
    "op_seq_len = 200    # total num of characters in output test sequence\n",
    "load_chk = False    # load weights from save_path directory to continue training\n",
    "save_path = \"./preTrained/CharRNN_shakespeare.pth\"\n",
    "data_path = \"data/shakespear_dataset.txt\"\n",
    "#######################################\n",
    "\n",
    "# load the text file\n",
    "data = open(data_path, 'r').read()\n",
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Data has {} characters, {} unique\".format(data_size, vocab_size))\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor([17], device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e85c4ab1c9cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchar_to_ix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# data tensor on device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: tensor([17], device='cuda:0')"
     ]
    }
   ],
   "source": [
    "# char to index and index to char maps\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# convert data from chars to indices\n",
    "data = list(data)\n",
    "for i, ch in enumerate(data):\n",
    "    data[i] = char_to_ix[ch]\n",
    "\n",
    "# data tensor on device\n",
    "data = torch.tensor(data).to(device)\n",
    "data = torch.unsqueeze(data, dim=1)\n",
    "\n",
    "# model instance\n",
    "rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# load checkpoint if True\n",
    "if load_chk:\n",
    "    rnn.load_state_dict(torch.load(save_path))\n",
    "    print(\"Model loaded successfully !!\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "# loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-aa00f38c239f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# compute loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-69b8f60fabb9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_seq, hidden_state)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for i_epoch in range(1, epochs+1):\n",
    "\n",
    "    # random starting point (1st 100 chars) from data to begin\n",
    "    data_ptr = np.random.randint(100)\n",
    "    n = 0\n",
    "    running_loss = 0\n",
    "    hidden_state = None\n",
    "\n",
    "    while True:\n",
    "        input_seq = data[data_ptr : data_ptr+seq_len]\n",
    "        target_seq = data[data_ptr+1 : data_ptr+seq_len+1]\n",
    "\n",
    "        # forward pass\n",
    "        output, hidden_state = rnn(input_seq, hidden_state)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # compute gradients and take optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the data pointer\n",
    "        data_ptr += seq_len\n",
    "        n +=1\n",
    "\n",
    "        # if at end of data : break\n",
    "        if data_ptr + seq_len + 1 > data_size:\n",
    "            break\n",
    "\n",
    "    # print loss and save weights after every epoch\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(i_epoch, running_loss/n))\n",
    "    #torch.save(rnn.state_dict(), save_path)\n",
    "\n",
    "    # sample / generate a text sequence after every epoch\n",
    "    data_ptr = 0\n",
    "    hidden_state = None\n",
    "\n",
    "    # random character from data to begin\n",
    "    rand_index = np.random.randint(data_size-1)\n",
    "    input_seq = data[rand_index : rand_index+1]\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "    while True:\n",
    "        # forward pass\n",
    "        output, hidden_state = rnn(input_seq, hidden_state)\n",
    "\n",
    "        # construct categorical distribution and sample a character\n",
    "        output = F.softmax(torch.squeeze(output), dim=0)\n",
    "        dist = Categorical(output)\n",
    "        index = dist.sample()\n",
    "\n",
    "        # print the sampled character\n",
    "        print(ix_to_char[index.item()], end='')\n",
    "\n",
    "        # next input is current output\n",
    "        input_seq[0][0] = index.item()\n",
    "        data_ptr += 1\n",
    "\n",
    "        if data_ptr > op_seq_len:\n",
    "            break\n",
    "\n",
    "    print(\"\\n----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
